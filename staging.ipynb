{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import datetime as dt\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2, pickle\n",
    "from psycopg2 import Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "user=\"postgres\"\n",
    "password=\"Vijay42****@\"\n",
    "host=\"localhost\"\n",
    "port=\"5432\"\n",
    "database=\"smdvault12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileReader():  \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.__filename = \"\"\n",
    "    \n",
    "    def _findField(self,file, fieldName, allLines = []):\n",
    "        \n",
    "        if not allLines:\n",
    "            allLines = file.readlines()\n",
    "             \n",
    "        \n",
    "        for currentLine in allLines:\n",
    "            if fieldName in currentLine:\n",
    "                currentLine = currentLine.split(fieldName)\n",
    "                return  ''.join(currentLine[1])\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "        \n",
    "    def _findFieldPosition(self,file, fieldName):\n",
    "        currentLine = file.readline()\n",
    "        if fieldName in currentLine:\n",
    "            return  None\n",
    "        else:\n",
    "            return self._findFieldPosition(file, fieldName)\n",
    "\n",
    "        \n",
    "    def readVMFile(self, file):\n",
    "\n",
    "        metadata = {}\n",
    "        fields = ['ID','Name','Age','Sex','AnalyzeMode','Pre Time[s]','Post Time[s]','Recovery Time[s]','Base Time[s]','Date','Mode','Wave[nm]','Sampling Period[s]','StimType','Stim Time[s]','Repeat Count']\n",
    "        for field in fields:\n",
    "            if 'Stim Time[s]' != field:\n",
    "                if 'Wave[nm]' != field:\n",
    "                    metadata[field] = self._findField(file, field).replace('\\n','').lstrip(',').rstrip(',')\n",
    "                    file.seek(0)\n",
    "                else:    \n",
    "                    metadata[field] = self._findField(file, field).replace('\\n','').lstrip(',').rstrip(',').split(',')\n",
    "                    file.seek(0)\n",
    "            else:\n",
    "                self._findFieldPosition(file, field)\n",
    "                fieldList = file.readline().replace('\\n','').replace(',,','').split(\",\")\n",
    "                file.seek(0)\n",
    "                fieldDict ={}\n",
    "                for i in range(len(fieldList) - 1):\n",
    "                    if i % 2 == 0:\n",
    "                        fieldDict[fieldList[i]] = fieldList[i+1]\n",
    "                \n",
    "                metadata[field] = fieldDict\n",
    "\n",
    "        self._findFieldPosition(file,'Data')\n",
    "        data = pd.read_csv(file)\n",
    "            \n",
    "\n",
    "        \n",
    "        return (metadata,data)\n",
    "    \n",
    "    def getParameters(self, file, field, params):\n",
    "        parameters = {}\n",
    "        self._findFieldPosition(file, field)\n",
    "        headerLines = file.readlines()   \n",
    "        for param in params:\n",
    "            parameters[param] = self._findField(file, param, headerLines).replace('\\n','').replace('\\\"','').replace('=','').replace('\\t',',')\n",
    "        \n",
    "        file.seek(0)\n",
    "        return parameters\n",
    "\n",
    "    \n",
    "    def getArray(self, file, field, params, arrayFields):\n",
    "        fieldArray = {}\n",
    "        fieldArray = self.getParameters(file, field, params)\n",
    "        \n",
    "        for fieldValue in arrayFields:\n",
    "            self._findFieldPosition(file, fieldValue)\n",
    "            arrayValue = np.array([])\n",
    "            currentLine = file.readline().replace('\\n','')\n",
    "            while \"#\" not in currentLine:\n",
    "                currentLine = currentLine.split('\\t')\n",
    "                if arrayValue.shape[0] == 0:\n",
    "                    arrayValue = np.array(currentLine)\n",
    "                else:\n",
    "                    arrayValue = np.vstack((arrayValue, currentLine))\n",
    "                currentLine = file.readline().replace('\\n','')\n",
    "            fieldArray[fieldValue] = arrayValue\n",
    "            \n",
    "        \n",
    "        return fieldArray\n",
    "    \n",
    "    \n",
    "    def readPreAutismMetaData(self, file):\n",
    "        \n",
    "        metadata = {}\n",
    "        \n",
    "        GeneralInfoFields = ['FileName','Date','Time','Device','Source','Mod','APD','NIRStar','Subject']\n",
    "        ImagingParametersFields = ['Sources','Detectors','ShortDetectors','ShortBundles','ShortDetIndex','Steps','Wavelengths','TrigIns','TrigOuts','AnIns','SamplingRate','Mod Amp','Threshold']\n",
    "        ParadigmFields = ['StimulusType']\n",
    "        ExperimentNotesFields = ['Notes']\n",
    "        GainSettingsFields = []\n",
    "        GainSettingsArrayFields = ['Gains']\n",
    "        MarkersFields = []\n",
    "        MarkersArrayFields = ['Events']\n",
    "        DataStructureFields = ['S-D-Key']\n",
    "        DataStructureArrayFields = ['S-D-Mask']\n",
    "        DarkNoiseFields = []\n",
    "        DarkNoiseArrayFields = ['Wavelength1','Wavelength2']\n",
    "        ChannelsDistanceFields = ['ChanDis']\n",
    "        \n",
    "            \n",
    "        metadata[\"GeneralInfo\"] = self.getParameters(file, \"GeneralInfo\", GeneralInfoFields)\n",
    "        metadata[\"ImagingParameters\"] = self.getParameters(file, \"ImagingParameters\", ImagingParametersFields)\n",
    "        metadata[\"Paradigm\"] = self.getParameters(file, \"Paradigm\", ParadigmFields)\n",
    "        metadata[\"ExperimentNotes\"] = self.getParameters(file, \"ExperimentNotes\", ExperimentNotesFields)\n",
    "        metadata[\"GainSettings\"] = self.getArray(file, \"GainSettings\", GainSettingsFields, GainSettingsArrayFields)\n",
    "        metadata[\"Markers\"] = self.getArray(file, \"Markers\", MarkersFields, MarkersArrayFields)\n",
    "        metadata[\"DataStructure\"] = self.getArray(file, \"DataStructure\", DataStructureFields, DataStructureArrayFields)\n",
    "        metadata[\"DarkNoise\"] = self.getArray(file, \"DarkNoise\", DarkNoiseFields, DarkNoiseArrayFields)\n",
    "        metadata[\"ChannelsDistance\"] = self.getParameters(file, \"ChannelsDistance\", ChannelsDistanceFields)\n",
    "        \n",
    "        return metadata    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileTransformer():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.__filename = \"\"\n",
    "        \n",
    "    def transformVMFile(self, fileName, metaData, data):\n",
    "\n",
    "        transformData = {}\n",
    "        dates = []\n",
    "        keys = []\n",
    "        values = []\n",
    "        samplePeriods = []\n",
    "            \n",
    "        for meta in metaData:\n",
    "            dates.append(meta['Date'])\n",
    "            samplePeriods.append(meta['Sampling Period[s]'])\n",
    "            keys.append(list(meta.keys()))\n",
    "            values.append(list(meta.values()))\n",
    "\n",
    "        dateSamplingTimeDF = pd.DataFrame({'date': dates, 'samplePeriod': samplePeriods})\n",
    "        dateSamplingTimeDF['samplePeriod'] = dateSamplingTimeDF['samplePeriod'].astype(float)\n",
    "        dateSamplingTimeDF['date'] =  pd.to_datetime(dateSamplingTimeDF['date'], format=\"%d/%m/%Y %H:%M:%S\")\n",
    "        \n",
    "        experimentTitle=[]\n",
    "        acronym=[]\n",
    "        for file in fileName:\n",
    "            title = file.split('\\\\')\n",
    "            acro = title[1].split('_')\n",
    "            experimentTitle.append(title[1].replace('.csv',''))\n",
    "            acronym.append(acro[1])\n",
    "        \n",
    "        SatMetaDataKeyValuePairDF = pd.DataFrame([])\n",
    "        SatMetaDataKeyValuePairDF['sequence'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        SatMetaDataKeyValuePairDF['key'] = pd.Series(keys)\n",
    "        SatMetaDataKeyValuePairDF['value'] = pd.Series(values)\n",
    "        SatMetaDataKeyValuePairDF = SatMetaDataKeyValuePairDF.set_index(['sequence']).apply(pd.Series.explode).reset_index()\n",
    "        SatMetaDataKeyValuePairDF['value'] = pickle.dumps(SatMetaDataKeyValuePairDF['value']) # SatMetaDataKeyValuePairDF['value'].astype('str')\n",
    "        \n",
    "        transformData['SatMetaDataKeyValuePair'] = SatMetaDataKeyValuePairDF\n",
    "\n",
    "        HubMetaDataDF = pd.DataFrame([])\n",
    "        HubMetaDataDF['sequence'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        \n",
    "        transformData['HubMetaData'] = HubMetaDataDF\n",
    "\n",
    "\n",
    "            \n",
    "        SatExperimentTitleDF = pd.DataFrame([])\n",
    "        SatExperimentTitleDF['sequence'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        SatExperimentTitleDF['title']= pd.Series(experimentTitle)\n",
    "        \n",
    "        transformData['SatExperimentTitle'] = SatExperimentTitleDF\n",
    "        \n",
    "        \n",
    "        SatExperimentAcronymDF = pd.DataFrame([])\n",
    "        SatExperimentAcronymDF['sequence'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        SatExperimentAcronymDF['acronym'] = pd.Series(acronym)\n",
    "        \n",
    "        transformData['SatExperimentAcronym'] = SatExperimentAcronymDF\n",
    "        \n",
    "        \n",
    "        HubExperimentDF = pd.DataFrame([])\n",
    "        HubExperimentDF['sequence'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        \n",
    "        transformData['HubExperiment'] = HubExperimentDF\n",
    "        \n",
    "        HubExperimentalUnitDF = pd.DataFrame([])\n",
    "        HubExperimentalUnitDF['sequence'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        \n",
    "        transformData['HubExperimentalUnit'] = HubExperimentalUnitDF\n",
    "        \n",
    "        identities=[]\n",
    "        for meta in metaData:\n",
    "            identities.append(meta['ID'])\n",
    "        \n",
    "        SatExperimentalUnitIdentifierDF = pd.DataFrame([])\n",
    "        SatExperimentalUnitIdentifierDF['sequence'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        SatExperimentalUnitIdentifierDF['ID'] = pd.Series(identities)\n",
    "        \n",
    "        transformData['SatExperimentalUnitIdentifier'] = SatExperimentalUnitIdentifierDF\n",
    "        \n",
    "        names=[]\n",
    "        for meta in metaData:\n",
    "            names.append(meta['Name'])\n",
    "            \n",
    "        HubSubjectDF = pd.DataFrame([])\n",
    "        HubSubjectDF['sequence'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        HubSubjectDF['name'] = pd.Series(names)\n",
    "        \n",
    "        transformData['HubSubject'] = HubSubjectDF\n",
    "        \n",
    "        ages=[]\n",
    "        for meta in metaData:\n",
    "            ages.append(int(meta['Age'].lstrip().rstrip().replace('y',''))) \n",
    "            \n",
    "        SatSubjectAgeDF = pd.DataFrame([])\n",
    "        SatSubjectAgeDF['sequence'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        SatSubjectAgeDF['age'] = pd.Series(ages)\n",
    "        \n",
    "        transformData['SatSubjectAge'] = SatSubjectAgeDF\n",
    "        \n",
    "        SatSubjectNameDF = pd.DataFrame([])\n",
    "        SatSubjectNameDF['sequence'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        SatSubjectNameDF['name'] = pd.Series(names)\n",
    "        \n",
    "        transformData['SatSubjectName'] = SatSubjectNameDF\n",
    "        \n",
    "        ParticipatesInDF = pd.DataFrame([])\n",
    "        ParticipatesInDF['sequence'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        ParticipatesInDF['experimentalunit'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        ParticipatesInDF['experiment'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        \n",
    "        transformData['ParticipatesIn'] = ParticipatesInDF\n",
    "        \n",
    "        SatFactorDF = pd.DataFrame([])\n",
    "        SatFactorDF['sequence'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        SatFactorDF['experiment'] = SatFactorDF['sequence']\n",
    "        SatFactorDF['isCofactor'] = pd.Series([False for x in range(len(SatFactorDF.index))])\n",
    "        SatFactorDF['name'] = pd.Series([list(['Visual Stimulus','Motor Stimulus']) for x in range(len(SatFactorDF.index))])\n",
    "        SatFactorDF = pd.merge(SatFactorDF, SatExperimentAcronymDF, how = 'inner', left_on='sequence', right_on='sequence')\n",
    "        \n",
    "        \n",
    "        \n",
    "        levelValueconditions = [\n",
    "            (SatFactorDF['acronym'] == \"ViMo\"),\n",
    "            (SatFactorDF['acronym'] == \"Viso\"),\n",
    "            (SatFactorDF['acronym'] == \"Moto\"),\n",
    "            (SatFactorDF['acronym'] == \"Rest\")\n",
    "        ]\n",
    "        levelValues = [\"True,True\",\"True,False\",\"False,True\",\"False,False\"]\n",
    "        SatFactorDF['levelValue'] = np.select(levelValueconditions, levelValues)\n",
    "        SatFactorDF = SatFactorDF[['sequence','name','levelValue','experiment','isCofactor']]\n",
    "        SatFactorDF['levelValue'] = SatFactorDF['levelValue'].apply(lambda x: x.split(','))\n",
    "        SatFactorDF = SatFactorDF.set_index(['sequence','experiment','isCofactor']).apply(pd.Series.explode).reset_index()\n",
    "        SatFactorDF['sequence'] = SatFactorDF['sequence'] + SatFactorDF['name']\n",
    "        HubFactorDF = SatFactorDF[['sequence','experiment','isCofactor']]\n",
    "        \n",
    "        transformData['HubFactor'] = HubFactorDF\n",
    "        \n",
    "        SatFactorNameDF = SatFactorDF[['sequence','name']]\n",
    "        \n",
    "        transformData['SatFactorName'] = SatFactorNameDF\n",
    "        \n",
    "        SatFactorLevelDF = SatFactorDF[['sequence','levelValue']]\n",
    "        \n",
    "        transformData['SatFactorLevel'] = SatFactorLevelDF\n",
    "        \n",
    "        HubTreatmentDF = pd.DataFrame([])\n",
    "        HubTreatmentDF['sequence'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        HubTreatmentDF['experiment'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        \n",
    "        transformData['HubTreatment'] = HubTreatmentDF\n",
    "        \n",
    "        SatTreatmentFactorLevelDF = SatFactorDF[['sequence','experiment']]\n",
    "        \n",
    "        transformData['SatTreatmentFactorLevel'] = SatTreatmentFactorLevelDF\n",
    "        \n",
    "        HubGroupDF = pd.DataFrame([])\n",
    "        HubGroupDF['sequence'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        HubGroupDF['treatment'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        \n",
    "        transformData['HubGroup'] = HubGroupDF\n",
    "        \n",
    "        AssignedToDF= pd.DataFrame([])\n",
    "        AssignedToDF['sequence'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        AssignedToDF['experimentalUnit'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        AssignedToDF['group'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        \n",
    "        transformData['AssignedTo'] = AssignedToDF\n",
    "        \n",
    "        levelNames = ['Visual and Motor Stimulus','Visual Stimulus','Motor Stimulus','Rest']\n",
    "        SatGroupNameDF = SatExperimentAcronymDF[['sequence','acronym']]\n",
    "        SatGroupNameDF['name'] = np.select(levelValueconditions, levelNames)\n",
    "        SatGroupNameDF = SatGroupNameDF[['sequence','name']]\n",
    "        \n",
    "        transformData['SatGroupName'] =SatGroupNameDF\n",
    "        \n",
    "        SatSessionNameDF = SatGroupNameDF\n",
    "        \n",
    "        transformData['SatSessionName'] = SatSessionNameDF\n",
    "        \n",
    "        HubSessionDF = pd.DataFrame([])\n",
    "        HubSessionDF['sequence'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        \n",
    "        transformData['HubSession'] = HubSessionDF\n",
    "        \n",
    "        AttendsSessionDF = pd.DataFrame([])\n",
    "        AttendsSessionDF['sequence'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        AttendsSessionDF['experimentalUnit'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        AttendsSessionDF['group'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        AttendsSessionDF['session'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        \n",
    "        transformData['AttendsSession'] = AttendsSessionDF\n",
    "        \n",
    "        SessionMetaDataDF = pd.DataFrame([])\n",
    "        SessionMetaDataDF['sequence'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        SessionMetaDataDF['session'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        SessionMetaDataDF['metadata'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        \n",
    "        transformData['SessionMetaData'] = SessionMetaDataDF\n",
    "        \n",
    "        HubObservationDF = pd.DataFrame([])\n",
    "        HubObservationDF['sequence'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        HubObservationDF['collectedAtSession'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        \n",
    "        transformData['HubObservation'] = HubObservationDF\n",
    "        \n",
    "        ObservationMetaDataDF = pd.DataFrame([])\n",
    "        ObservationMetaDataDF['sequence'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        ObservationMetaDataDF['observation'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        ObservationMetaDataDF['metadata'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        \n",
    "        transformData['ObservationMetaData'] = ObservationMetaDataDF\n",
    "        \n",
    "        SatObservationNameDF = pd.DataFrame([])\n",
    "        SatObservationNameDF['sequence'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        SatObservationNameDF['name'] = pd.Series(experimentTitle)\n",
    "        \n",
    "        transformData['SatObservationName'] = SatObservationNameDF\n",
    "        \n",
    "        arrayData=[]\n",
    "        timestampData=[]\n",
    "        for dataValue, date, samplingRate in zip(data, dateSamplingTimeDF['date'].tolist(), dateSamplingTimeDF['samplePeriod'].tolist()):\n",
    "            if 'MES' in fileName[0]:\n",
    "                dataDF = dataValue\n",
    "                dataDF['date'] = date\n",
    "                dataDF['date'] = dataDF['date'].dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "                dataDF['samplingRate'] = samplingRate\n",
    "                dataDF['sampleNumber'] = np.arange(dataDF.shape[0])\n",
    "                dataDF['timestamps'] = pd.to_datetime(dataDF['date']) + pd.to_timedelta((dataDF['samplingRate'] * dataDF['sampleNumber']), unit='s')    # .apply(lambda x: x + datetime.timedelta(seconds = x['samplingRate'] * x['sampleNumber']))\n",
    "                dataDF['timestamps']=dataDF['timestamps'].astype(str)\n",
    "               \n",
    "                timeData = dataDF['timestamps'].tolist()\n",
    "                \n",
    "                timeDataFormatted = []\n",
    "                for t in timeData:\n",
    "                    timeDataFormatted.append(dt.datetime.strptime(t,'%Y-%m-%d %H:%M:%S.%f'))\n",
    "                \n",
    "                arrayData.append(dataValue.loc[:,['CH1(698.1)','CH1(828.7)','CH2(697.1)','CH2(828.2)','CH3(698.1)','CH3(828.7)','CH4(698.3)','CH4(828.4)','CH5(697.1)','CH5(828.2)','CH6(698.3)','CH6(828.4)','CH7(698.3)','CH7(828.4)','CH8(697.5)','CH8(828.7)','CH9(698.3)','CH9(828.4)','CH10(697.9)','CH10(829.0)','CH11(697.5)','CH11(828.7)','CH12(697.9)','CH12(829.0)','CH13(698.7)','CH13(828.2)','CH14(698.2)','CH14(827.5)','CH15(698.7)','CH15(828.2)','CH16(697.7)','CH16(828.6)','CH17(698.2)','CH17(827.5)','CH18(697.7)','CH18(828.6)','CH19(697.7)','CH19(828.6)','CH20(698.4)','CH20(828.9)','CH21(697.7)','CH21(828.6)','CH22(697.1)','CH22(828.8)','CH23(698.4)','CH23(828.9)','CH24(697.1)','CH24(828.8)']].values.tolist())\n",
    "                timestampData.append(timeData)\n",
    "            else:\n",
    "                dataDF = dataValue\n",
    "                dataDF['date'] = date\n",
    "                dataDF['date'] = dataDF['date'].dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "                dataDF['samplingRate'] = samplingRate\n",
    "                dataDF['sampleNumber'] = np.arange(dataDF.shape[0])\n",
    "                dataDF['timestamps'] = pd.to_datetime(dataDF['date']) + pd.to_timedelta((dataDF['samplingRate'] * dataDF['sampleNumber']), unit='s')\n",
    "                dataDF['timestamps']=dataDF['timestamps'].astype(str)\n",
    "                \n",
    "                timeData = dataDF['timestamps'].tolist()\n",
    "                \n",
    "                timeDataFormatted = []\n",
    "                for t in timeData:\n",
    "                    timeDataFormatted.append(dt.datetime.strptime(t,'%Y-%m-%d %H:%M:%S.%f'))\n",
    "                \n",
    "                arrayData.append(dataValue.loc[:,['CH1','CH2','CH3','CH4','CH5','CH6','CH7','CH8','CH9','CH10','CH11','CH12','CH13','CH14','CH15','CH16','CH17','CH18','CH19','CH20','CH21','CH22','CH23','CH24']].values.tolist())\n",
    "                timestampData.append(timeData)\n",
    "        \n",
    "        SatObservationValueDF = pd.DataFrame([])\n",
    "        SatObservationValueDF['sequence'] = pd.Series(a + '_' +b for a, b in zip(dates, experimentTitle))\n",
    "        SatObservationValueDF['value'] = pd.Series(arrayData)\n",
    "        SatObservationValueDF['timestamps'] = pd.Series(timestampData)\n",
    "        \n",
    "        transformData['SatObservationValue'] = SatObservationValueDF\n",
    "        \n",
    "        return transformData\n",
    "    \n",
    "    \n",
    "    def transformPreAutismFile(self,preAutismFileNames, preAutismMetaData, preAutismData, preAutismWavelengthOneData, preAutismWavelengthTwoData, preAutismEventonsData):\n",
    "        \n",
    "        transformData = {}\n",
    "\n",
    "        \n",
    "        def recursive_items_key(dictionary):\n",
    "            for key, value in dictionary.items():\n",
    "                if type(value) is dict:\n",
    "                     yield from recursive_items_key(value)\n",
    "                else:\n",
    "                    yield (key, value)\n",
    "\n",
    "        def getKeyValueArrays(input):\n",
    "            keyArray = []\n",
    "            valueArray =[]\n",
    "            x = input\n",
    "            for key, value in recursive_items_key(x):\n",
    "                keyArray.append(key)\n",
    "                valueArray.append(value)\n",
    "            \n",
    "            return keyArray,valueArray\n",
    "        \n",
    "        \n",
    "        fileName = []\n",
    "        date = []\n",
    "        time = []\n",
    "        samplingRates = []\n",
    "    \n",
    "        simplifiedMetaDataList =[]\n",
    "        for metaData in preAutismMetaData:\n",
    "            simplifiedMetaData = {}\n",
    "            for key, value in recursive_items_key(metaData):\n",
    "                simplifiedMetaData[key] = value\n",
    "            simplifiedMetaDataList.append(simplifiedMetaData)\n",
    "        \n",
    "        for meta in simplifiedMetaDataList:\n",
    "            fileName.append(meta['FileName'])\n",
    "            date.append(meta['Date'])\n",
    "            time.append(meta['Time'])\n",
    "            samplingRates.append(meta['SamplingRate'])\n",
    "        \n",
    "        preAutismDF = pd.DataFrame([])\n",
    "        preAutismDF['sequence'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        preAutismDF['preAutismFileNames'] = pd.Series(preAutismFileNames)\n",
    "        preAutismDF['preAutismMetaData'] = pd.Series(preAutismMetaData)\n",
    "        preAutismDF['preAutismData'] = pd.Series(preAutismData)\n",
    "        preAutismDF['preAutismWavelengthOneData'] = pd.Series(preAutismWavelengthOneData)\n",
    "        preAutismDF['preAutismWavelengthTwoData'] = pd.Series(preAutismWavelengthTwoData)\n",
    "        preAutismDF['preAutismEventonsData'] = pd.Series(preAutismEventonsData)\n",
    "        \n",
    "        \n",
    "        SatMetaDataKeyValuePairDF = pd.DataFrame([])\n",
    "        SatMetaDataKeyValuePairDF['sequence'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        SatMetaDataKeyValuePairDF['keyValue'] = preAutismDF['preAutismMetaData'].apply(lambda x: getKeyValueArrays(x))\n",
    "        SatMetaDataKeyValuePairDF[['key','value']] = pd.DataFrame(SatMetaDataKeyValuePairDF['keyValue'].to_list(), index=SatMetaDataKeyValuePairDF.index)\n",
    "        SatMetaDataKeyValuePairDF = SatMetaDataKeyValuePairDF[['sequence','key','value']] \n",
    "        SatMetaDataKeyValuePairDF = SatMetaDataKeyValuePairDF.set_index(['sequence']).apply(pd.Series.explode).reset_index()\n",
    "        SatMetaDataKeyValuePairDF['value'] = pickle.dumps(SatMetaDataKeyValuePairDF['value'])\n",
    "        \n",
    "        transformData['SatMetaDataKeyValuePair'] = SatMetaDataKeyValuePairDF\n",
    "\n",
    "        HubMetaDataDF = pd.DataFrame([])\n",
    "        HubMetaDataDF['sequence'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        \n",
    "        transformData['HubMetaData'] = HubMetaDataDF\n",
    "     \n",
    "        SatExperimentTitleDF = pd.DataFrame([])\n",
    "        SatExperimentTitleDF['sequence'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        SatExperimentTitleDF['title']= preAutismDF['preAutismFileNames'].apply(lambda x: x.split('\\\\')[1])\n",
    "\n",
    "        transformData['SatExperimentTitle'] = SatExperimentTitleDF\n",
    "        \n",
    "            \n",
    "        SatExperimentAcronymDF = pd.DataFrame([])\n",
    "        SatExperimentAcronymDF['sequence'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        SatExperimentAcronymDF['acronym'] = preAutismDF['preAutismFileNames'].apply(lambda x: x.split('\\\\')[1].replace('Autism','').replace('Conversation',''))\n",
    "        \n",
    "        transformData['SatExperimentAcronym'] = SatExperimentAcronymDF\n",
    "        \n",
    "        \n",
    "        HubExperimentDF = pd.DataFrame([])\n",
    "        HubExperimentDF['sequence'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        \n",
    "        transformData['HubExperiment'] = HubExperimentDF\n",
    "        \n",
    "        HubExperimentalUnitDF = pd.DataFrame([])\n",
    "        HubExperimentalUnitDF['sequence'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        \n",
    "        transformData['HubExperimentalUnit'] = HubExperimentalUnitDF\n",
    "        \n",
    "        SatExperimentalUnitIdentifierDF = pd.DataFrame([])\n",
    "        SatExperimentalUnitIdentifierDF['sequence'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        SatExperimentalUnitIdentifierDF['ID'] = preAutismDF['preAutismFileNames'].apply(lambda x: x.split('\\\\')[1].split(\"_\")[0])\n",
    "        \n",
    "        transformData['SatExperimentalUnitIdentifier'] = SatExperimentalUnitIdentifierDF\n",
    "            \n",
    "        HubSubjectDF = pd.DataFrame([])\n",
    "        HubSubjectDF['sequence'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        HubSubjectDF['name'] = preAutismDF['preAutismFileNames'].apply(lambda x: x.split('\\\\')[1].split(\"-\")[0])\n",
    "        \n",
    "        transformData['HubSubject'] = HubSubjectDF\n",
    "            \n",
    "        SatSubjectAgeDF = pd.DataFrame([])\n",
    "        SatSubjectAgeDF['sequence'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        SatSubjectAgeDF['age'] = pd.Series([0 for x in range(len(SatSubjectAgeDF.index))])\n",
    "        \n",
    "        transformData['SatSubjectAge'] = SatSubjectAgeDF\n",
    "        \n",
    "        SatSubjectNameDF = pd.DataFrame([])\n",
    "        SatSubjectNameDF['sequence'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        SatSubjectNameDF['name'] = preAutismDF['preAutismFileNames'].apply(lambda x: x.split('\\\\')[1].split(\"-\")[0])\n",
    "        \n",
    "        transformData['SatSubjectName'] = SatSubjectNameDF\n",
    "        \n",
    "        ParticipatesInDF = pd.DataFrame([])\n",
    "        ParticipatesInDF['sequence'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        ParticipatesInDF['experimentalunit'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        ParticipatesInDF['experiment'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        \n",
    "        transformData['ParticipatesIn'] = ParticipatesInDF\n",
    "        \n",
    "        SatFactorDF = pd.DataFrame([])\n",
    "        SatFactorDF['sequence'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        SatFactorDF['experiment'] = SatFactorDF['sequence']\n",
    "        SatFactorDF['isCofactor'] = pd.Series([False for x in range(len(SatFactorDF.index))])\n",
    "        SatFactorDF['name'] = pd.Series(['Conversation' for x in range(len(SatFactorDF.index))])\n",
    "        SatFactorDF['preAutismFileNames']= pd.Series(preAutismFileNames)\n",
    "\n",
    "        SatFactorDF['levelValue'] = SatFactorDF['preAutismFileNames'].apply(lambda x: \"Normal\" if \"NormalConversation\" in x else \"Stressed\")\n",
    "        SatFactorDF = SatFactorDF[['sequence','name','levelValue','experiment','isCofactor']]\n",
    "        HubFactorDF = SatFactorDF[['sequence','experiment','isCofactor']]\n",
    "        \n",
    "        transformData['HubFactor'] = HubFactorDF\n",
    "        \n",
    "        SatFactorNameDF = SatFactorDF[['sequence','name']]\n",
    "        \n",
    "        transformData['SatFactorName'] = SatFactorNameDF\n",
    "        \n",
    "        SatFactorLevelDF = SatFactorDF[['sequence','levelValue']]\n",
    "        \n",
    "        transformData['SatFactorLevel'] = SatFactorLevelDF\n",
    "        \n",
    "        HubTreatmentDF = pd.DataFrame([])\n",
    "        HubTreatmentDF['sequence'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        HubTreatmentDF['experiment'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        \n",
    "        transformData['HubTreatment'] = HubTreatmentDF\n",
    "        \n",
    "        SatTreatmentFactorLevelDF = SatFactorDF[['sequence','experiment']]\n",
    "        \n",
    "        transformData['SatTreatmentFactorLevel'] = SatTreatmentFactorLevelDF\n",
    "        \n",
    "        HubGroupDF = pd.DataFrame([])\n",
    "        HubGroupDF['sequence'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        HubGroupDF['treatment'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        \n",
    "        transformData['HubGroup'] = HubGroupDF\n",
    "        \n",
    "        AssignedToDF= pd.DataFrame([])\n",
    "        AssignedToDF['sequence'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        AssignedToDF['experimentalUnit'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        AssignedToDF['group'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        \n",
    "        transformData['AssignedTo'] = AssignedToDF\n",
    "\n",
    "        SatGroupNameDF = SatFactorDF[['sequence','levelValue']]\n",
    "        SatGroupNameDF = SatGroupNameDF.rename(columns = {'levelValue' : 'name'})\n",
    "        SatGroupNameDF = SatGroupNameDF[['sequence','name']]\n",
    "        \n",
    "        transformData['SatGroupName'] =SatGroupNameDF\n",
    "        \n",
    "        SatSessionNameDF = SatGroupNameDF\n",
    "        \n",
    "        transformData['SatSessionName'] = SatSessionNameDF\n",
    "        \n",
    "        HubSessionDF = pd.DataFrame([])\n",
    "        HubSessionDF['sequence'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        \n",
    "        transformData['HubSession'] = HubSessionDF\n",
    "        \n",
    "        AttendsSessionDF = pd.DataFrame([])\n",
    "        AttendsSessionDF['sequence'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        AttendsSessionDF['experimentalUnit'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        AttendsSessionDF['group'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        AttendsSessionDF['session'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        \n",
    "        transformData['AttendsSession'] = AttendsSessionDF\n",
    "        \n",
    "        SessionMetaDataDF = pd.DataFrame([])\n",
    "        SessionMetaDataDF['sequence'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        SessionMetaDataDF['session'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        SessionMetaDataDF['metadata'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        \n",
    "        transformData['SessionMetaData'] = SessionMetaDataDF\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        SatObservationNameDF = pd.DataFrame([])\n",
    "        SatObservationNameDF['initialSequence'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        SatObservationNameDF['name'] = SatExperimentTitleDF['title']\n",
    "        SatObservationNameDF['observationType'] = pd.Series([list(['data', 'wavelengthOneData', 'wavelengthTwoData', 'eventonsData']) for x in range(len(SatObservationNameDF.index))])\n",
    "        SatObservationNameDF = SatObservationNameDF.set_index(['initialSequence','name']).apply(pd.Series.explode).reset_index()\n",
    "        SatObservationNameDF['name'] = SatObservationNameDF['name'] + '_' + SatObservationNameDF['observationType']\n",
    "        SatObservationNameDF['name'] = SatObservationNameDF['name'].apply(lambda x: x.replace(\"Conversation\",\"\").replace(\"Autism\",\"\"))\n",
    "        SatObservationNameDF['sequence'] = SatObservationNameDF['initialSequence'] + '_' + SatObservationNameDF['observationType']\n",
    "        \n",
    "        transformData['SatObservationName'] = SatObservationNameDF\n",
    "        \n",
    "        HubObservationDF = pd.DataFrame([])\n",
    "        HubObservationDF['sequence'] = SatObservationNameDF['sequence']\n",
    "        HubObservationDF['collectedAtSession'] = SatObservationNameDF['initialSequence']\n",
    "        \n",
    "        transformData['HubObservation'] = HubObservationDF\n",
    "        \n",
    "        \n",
    "        ObservationMetaDataDF = pd.DataFrame([])\n",
    "        ObservationMetaDataDF['sequence'] = SatObservationNameDF['initialSequence']\n",
    "        ObservationMetaDataDF['observation'] = SatObservationNameDF['sequence']\n",
    "        ObservationMetaDataDF['metadata'] = SatObservationNameDF['initialSequence']\n",
    "        \n",
    "        transformData['ObservationMetaData'] = ObservationMetaDataDF\n",
    "        \n",
    "        SatObservationTimeStampsDF = pd.DataFrame([])\n",
    "        SatObservationTimeStampsDF['initialSequence'] = pd.Series(a + '_' + b + '_'+ c for a, b, c in zip(fileName, date, time))\n",
    "        SatObservationTimeStampsDF['fileName'] = pd.Series(fileName)\n",
    "        SatObservationTimeStampsDF['fileName'] = SatObservationTimeStampsDF['fileName'].apply(lambda x : x.replace(\"NIRS-\",\"\").split(\"_\")[0])\n",
    "        SatObservationTimeStampsDF['time'] = pd.Series(time)\n",
    "        SatObservationTimeStampsDF['time'] = SatObservationTimeStampsDF['fileName'] + ' ' + SatObservationTimeStampsDF['time']\n",
    "        SatObservationTimeStampsDF['SamplingRate'] = pd.Series(samplingRates)\n",
    "        SatObservationTimeStampsDF['observationType'] = pd.Series([list(['data', 'wavelengthOneData', 'wavelengthTwoData', 'eventonsData']) for x in range(len(SatObservationTimeStampsDF.index))])\n",
    "        SatObservationTimeStampsDF = SatObservationTimeStampsDF.set_index(['initialSequence','fileName','time','SamplingRate']).apply(pd.Series.explode).reset_index()\n",
    "        SatObservationTimeStampsDF['sequence'] = SatObservationTimeStampsDF['initialSequence'] + '_' + SatObservationTimeStampsDF['observationType']\n",
    "        \n",
    "\n",
    "        def getTimestamps(startTime, shape, samplingRate):\n",
    "             \n",
    "            startDateTime = parser.parse(startTime)       \n",
    "            sampleNumbers = np.arange(shape)\n",
    "            samplingRate = float(samplingRate)\n",
    "            \n",
    "            timestamps = []\n",
    "            for sampleNumber in sampleNumbers:\n",
    "                timestamps.append(str((startDateTime + dt.timedelta(seconds= samplingRate * sampleNumber))))\n",
    "            return timestamps\n",
    "\n",
    "        SatObservationValueDataDF = SatObservationNameDF[SatObservationNameDF['name'].str.contains('data')].reset_index()\n",
    "        SatObservationValueDataDF = pd.concat([SatObservationValueDataDF, preAutismDF[['preAutismData']]], axis=1)\n",
    "        SatObservationValueDataDF = SatObservationValueDataDF[['sequence','preAutismData']]\n",
    "        SatObservationValueDataDF = SatObservationValueDataDF.rename(columns = {'preAutismData' : 'value'})\n",
    "        SatObservationValueDataDF = pd.merge(SatObservationValueDataDF, SatObservationTimeStampsDF, how = 'inner', left_on='sequence', right_on='sequence')\n",
    "        SatObservationValueDataDF['timestamps'] = SatObservationValueDataDF.apply(lambda x: getTimestamps(x['time'], x['value'].shape[0], x['SamplingRate']), axis = 1)\n",
    "        \n",
    "        SatObservationValueWavelengthOneDataDF = SatObservationNameDF[SatObservationNameDF['name'].str.contains('wavelengthOneData')].reset_index()\n",
    "        SatObservationValueWavelengthOneDataDF = pd.concat([SatObservationValueDataDF, preAutismDF[['preAutismWavelengthOneData']]], axis=1)\n",
    "        SatObservationValueWavelengthOneDataDF = SatObservationValueWavelengthOneDataDF[['sequence','preAutismWavelengthOneData']]\n",
    "        SatObservationValueWavelengthOneDataDF = SatObservationValueWavelengthOneDataDF.rename(columns = {'preAutismWavelengthOneData' : 'value'})\n",
    "        SatObservationValueWavelengthOneDataDF = pd.merge(SatObservationValueWavelengthOneDataDF, SatObservationTimeStampsDF, how = 'inner', left_on='sequence', right_on='sequence')\n",
    "        SatObservationValueWavelengthOneDataDF['timestamps'] = SatObservationValueWavelengthOneDataDF.apply(lambda x: getTimestamps(x['time'], x['value'].shape[0], x['SamplingRate']), axis = 1)\n",
    "        \n",
    "        SatObservationValueWavelengthTwoDataDF = SatObservationNameDF[SatObservationNameDF['name'].str.contains('wavelengthTwoData')].reset_index()\n",
    "        SatObservationValueWavelengthTwoDataDF = pd.concat([SatObservationValueDataDF, preAutismDF[['preAutismWavelengthTwoData']]], axis=1)\n",
    "        SatObservationValueWavelengthTwoDataDF = SatObservationValueWavelengthTwoDataDF[['sequence','preAutismWavelengthTwoData']]\n",
    "        SatObservationValueWavelengthTwoDataDF = SatObservationValueWavelengthTwoDataDF.rename(columns = {'preAutismWavelengthTwoData' : 'value'})\n",
    "        SatObservationValueWavelengthTwoDataDF = pd.merge(SatObservationValueWavelengthTwoDataDF, SatObservationTimeStampsDF, how = 'inner', left_on='sequence', right_on='sequence')\n",
    "        SatObservationValueWavelengthTwoDataDF['timestamps'] = SatObservationValueWavelengthTwoDataDF.apply(lambda x: getTimestamps(x['time'], x['value'].shape[0], x['SamplingRate']), axis = 1)    \n",
    "        \n",
    "        SatObservationValueDF = pd.concat([SatObservationValueDataDF, SatObservationValueWavelengthOneDataDF, SatObservationValueWavelengthTwoDataDF]).reset_index()\n",
    "        SatObservationValueDF['value'] = SatObservationValueDF['value'].apply(lambda x: x.values.tolist())\n",
    "        \n",
    "        transformData['SatObservationValue'] = SatObservationValueDF\n",
    "        \n",
    "        return transformData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileLoader():\n",
    "    \n",
    "    def loadDataToEnterpriseLayer(self,inputs):\n",
    "    \n",
    "        try:\n",
    "            connection = psycopg2.connect(user=user,password=password,host=host,port=port,database=database)\n",
    "\n",
    "            for input in inputs:\n",
    "                \n",
    "                HubMetaDataDF = input['HubMetaData']\n",
    "                for i in HubMetaDataDF.index:\n",
    "                    cursor = connection.cursor()\n",
    "                    query = f\"\"\"INSERT INTO \"HubMetaData\" (sequence,timestamp,source) VALUES (md5('%s'),current_timestamp,'{user}'); \"\"\" % (HubMetaDataDF['sequence'][i])\n",
    "                    cursor.execute(query)\n",
    "                    connection.commit()\n",
    "                \n",
    "                SatMetaDataKeyValuePairDF = input['SatMetaDataKeyValuePair']\n",
    "                for i in SatMetaDataKeyValuePairDF.index:\n",
    "                    cursor = connection.cursor()\n",
    "                    query = f\"\"\"INSERT INTO \"SatMetaDataKeyValuePair\" (sequence,timestamp,source,key,value) VALUES (md5('%s'),current_timestamp,'{user}','%s',%s); \"\"\" % (SatMetaDataKeyValuePairDF['sequence'][i],SatMetaDataKeyValuePairDF['key'][i], psycopg2.Binary(SatMetaDataKeyValuePairDF['value'][i]) )\n",
    "                    cursor.execute(query)\n",
    "                    connection.commit()\n",
    "                \n",
    "                # sql = \"\"\"SELECT sequence,timestamp,source,key,value FROM \"SatMetaDataKeyValuePair\";\"\"\"\n",
    "                # cursor = connection.cursor()\n",
    "                # cursor.execute(sql)\n",
    "                # for sequence,timestamp,source,key,value in cursor.fetchall(  ):\n",
    "                #     print(sequence,timestamp,source,key, pickle.loads(value))\n",
    "                \n",
    "                HubExperimentDF = input['HubExperiment']\n",
    "                for i in HubExperimentDF.index:\n",
    "                    cursor = connection.cursor()\n",
    "                    query = f\"\"\"INSERT INTO \"HubExperiment\" (sequence,timestamp,source) VALUES (md5('%s'),current_timestamp,'{user}'); \"\"\" % (HubExperimentDF['sequence'][i])\n",
    "                    cursor.execute(query)\n",
    "                    connection.commit()\n",
    "\n",
    "                SatExperimentTitleDF = input['SatExperimentTitle']\n",
    "                for i in SatExperimentTitleDF.index:\n",
    "                    cursor = connection.cursor()\n",
    "                    query = f\"\"\"INSERT INTO \"SatExperimentTitle\" (sequence,timestamp,source,title) VALUES (md5('%s'),current_timestamp,'{user}','%s'); \"\"\" % (SatExperimentTitleDF['sequence'][i],SatExperimentTitleDF['title'][i])\n",
    "                    cursor.execute(query)\n",
    "                    connection.commit()\n",
    "                    \n",
    "                SatExperimentAcronymDF = input['SatExperimentAcronym']\n",
    "                for i in SatExperimentAcronymDF.index:\n",
    "                    cursor = connection.cursor()\n",
    "                    query = f\"\"\"INSERT INTO \"SatExperimentAcronym\" (sequence,timestamp,source,acronym) VALUES (md5('%s'),current_timestamp,'{user}','%s'); \"\"\" % (SatExperimentAcronymDF['sequence'][i],SatExperimentAcronymDF['acronym'][i])\n",
    "                    cursor.execute(query)\n",
    "                    connection.commit()\n",
    "                \n",
    "                HubExperimentalUnitDF = input['HubExperimentalUnit']\n",
    "                for i in HubExperimentalUnitDF.index:\n",
    "                        cursor = connection.cursor()\n",
    "                        query = f\"\"\"INSERT INTO \"HubExperimentalUnit\" (sequence,timestamp,source) VALUES (md5('%s'),current_timestamp,'{user}'); \"\"\" % (HubExperimentalUnitDF['sequence'][i])\n",
    "                        cursor.execute(query)\n",
    "                        connection.commit()\n",
    "                \n",
    "                HubSubjectDF = input['HubSubject']\n",
    "                for i in HubSubjectDF.index:\n",
    "                        cursor = connection.cursor()\n",
    "                        query = f\"\"\"INSERT INTO \"HubSubject\" (sequence,timestamp,source,name) VALUES (md5('%s'),current_timestamp,'{user}','%s'); \"\"\" % (HubSubjectDF['sequence'][i],HubSubjectDF['name'][i])\n",
    "                        cursor.execute(query)\n",
    "                        connection.commit()\n",
    "                        \n",
    "                SatSubjectAgeDF = input['SatSubjectAge']\n",
    "                for i in SatSubjectAgeDF.index:\n",
    "                        cursor = connection.cursor()\n",
    "                        query = f\"\"\"INSERT INTO \"SatSubjectAge\" (sequence,timestamp,source,age) VALUES (md5('%s'),current_timestamp,'{user}',%s); \"\"\" % (SatSubjectAgeDF['sequence'][i],SatSubjectAgeDF['age'][i])\n",
    "                        cursor.execute(query)\n",
    "                        connection.commit()\n",
    "                \n",
    "                SatSubjectNameDF = input['SatSubjectName']\n",
    "                for i in SatSubjectNameDF.index:\n",
    "                        cursor = connection.cursor()\n",
    "                        query = f\"\"\"INSERT INTO \"SatSubjectName\" (sequence,timestamp,source,name) VALUES (md5('%s'),current_timestamp,'{user}','%s'); \"\"\" % (SatSubjectNameDF['sequence'][i],SatSubjectNameDF['name'][i])\n",
    "                        cursor.execute(query)\n",
    "                        connection.commit()\n",
    "                        \n",
    "                ParticipatesInDF = input['ParticipatesIn']\n",
    "                for i in ParticipatesInDF.index:\n",
    "                        cursor = connection.cursor()\n",
    "                        query = f\"\"\"INSERT INTO \"ParticipatesIn\" (sequence,timestamp,source,experimentalunit,experiment) VALUES (md5('%s'),current_timestamp,'{user}',md5('%s'),md5('%s')); \"\"\" % (ParticipatesInDF['sequence'][i],ParticipatesInDF['experimentalunit'][i],ParticipatesInDF['experiment'][i])\n",
    "                        cursor.execute(query)\n",
    "                        connection.commit()\n",
    "                        \n",
    "                SatExperimentalUnitIdentifierDF = input['SatExperimentalUnitIdentifier']\n",
    "                for i in SatExperimentalUnitIdentifierDF.index:\n",
    "                        cursor = connection.cursor()\n",
    "                        query = f\"\"\"INSERT INTO \"SatExperimentalUnitIdentifier\" (sequence,timestamp,source,\"ID\") VALUES (md5('%s'),current_timestamp,'{user}','%s'); \"\"\" % (SatExperimentalUnitIdentifierDF['sequence'][i],SatExperimentalUnitIdentifierDF['ID'][i])\n",
    "                        cursor.execute(query)\n",
    "                        connection.commit()\n",
    "                \n",
    "                HubFactorDF = input['HubFactor']\n",
    "                for i in HubFactorDF.index:\n",
    "                        cursor = connection.cursor()\n",
    "                        query = f\"\"\"INSERT INTO \"HubFactor\" (sequence,timestamp,source,experiment) VALUES (md5('%s'),current_timestamp,'{user}',md5('%s')); \"\"\" % (HubFactorDF['sequence'][i],HubFactorDF['experiment'][i])\n",
    "                        cursor.execute(query)\n",
    "                        connection.commit()     \n",
    "                        \n",
    "                SatFactorNameDF = input['SatFactorName']\n",
    "                for i in SatFactorNameDF.index:\n",
    "                        cursor = connection.cursor()\n",
    "                        query = f\"\"\"INSERT INTO \"SatFactorName\" (sequence,timestamp,source,name) VALUES (md5('%s'),current_timestamp,'{user}','%s'); \"\"\" % (SatFactorNameDF['sequence'][i],SatFactorNameDF['name'][i])\n",
    "                        cursor.execute(query)\n",
    "                        connection.commit()         \n",
    "\n",
    "        \n",
    "                SatFactorLevelDF = input['SatFactorLevel']          \n",
    "                for i in SatFactorLevelDF.index:\n",
    "                        cursor = connection.cursor()\n",
    "                        query = f\"\"\"INSERT INTO \"SatFactorLevel\" (sequence,timestamp,source,\"levelValue\") VALUES (md5('%s'),current_timestamp,'{user}','%s'); \"\"\" % (SatFactorLevelDF['sequence'][i],SatFactorLevelDF['levelValue'][i])\n",
    "                        cursor.execute(query)\n",
    "                        connection.commit()  \n",
    "                \n",
    "                HubTreatmentDF = input['HubTreatment']\n",
    "                for i in HubTreatmentDF.index:\n",
    "                        cursor = connection.cursor()\n",
    "                        query = f\"\"\"INSERT INTO \"HubTreatment\" (sequence,timestamp,source,experiment) VALUES (md5('%s'),current_timestamp,'{user}',md5('%s')); \"\"\" % (HubTreatmentDF['sequence'][i],HubTreatmentDF['experiment'][i])\n",
    "                        cursor.execute(query)\n",
    "                        connection.commit()  \n",
    "                \n",
    "                SatTreatmentFactorLevelDF = input['SatTreatmentFactorLevel']\n",
    "                for i in SatTreatmentFactorLevelDF.index:\n",
    "                        cursor = connection.cursor()\n",
    "                        query = f\"\"\"INSERT INTO \"SatTreatmentFactorLevel\" (sequence,timestamp,source,\"factorLevel\") VALUES (md5('%s'),current_timestamp,'{user}',md5('%s')); \"\"\" % (SatTreatmentFactorLevelDF['experiment'][i],SatTreatmentFactorLevelDF['sequence'][i])\n",
    "                        cursor.execute(query)\n",
    "                        connection.commit()  \n",
    "                \n",
    "                HubGroupDF = input['HubGroup']\n",
    "                for i in HubGroupDF.index:\n",
    "                        cursor = connection.cursor()\n",
    "                        query = f\"\"\"INSERT INTO \"HubGroup\" (sequence,timestamp,source,treatment) VALUES (md5('%s'),current_timestamp,'{user}',md5('%s')); \"\"\" % (HubGroupDF['sequence'][i],HubGroupDF['treatment'][i])\n",
    "                        cursor.execute(query)\n",
    "                        connection.commit()  \n",
    "                \n",
    "                SatGroupNameDF = input['SatGroupName']\n",
    "                for i in SatGroupNameDF.index:\n",
    "                        cursor = connection.cursor()\n",
    "                        query = f\"\"\"INSERT INTO \"SatGroupName\" (sequence,timestamp,source,name) VALUES (md5('%s'),current_timestamp,'{user}','%s'); \"\"\" % (SatGroupNameDF['sequence'][i],SatGroupNameDF['name'][i])\n",
    "                        cursor.execute(query)\n",
    "                        connection.commit() \n",
    "                \n",
    "                AssignedToDF = input['AssignedTo']\n",
    "                for i in AssignedToDF.index:\n",
    "                        cursor = connection.cursor()\n",
    "                        query = f\"\"\"INSERT INTO \"AssignedTo\" (sequence,timestamp,source,\"experimentalUnit\",\"group\") VALUES (md5('%s'),current_timestamp,'{user}',md5('%s'),md5('%s')); \"\"\" % (AssignedToDF['sequence'][i],AssignedToDF['experimentalUnit'][i],AssignedToDF['group'][i])\n",
    "                        cursor.execute(query)\n",
    "                        connection.commit() \n",
    "                        \n",
    "                HubSessionDF = input['HubSession']\n",
    "                for i in HubSessionDF.index:\n",
    "                        cursor = connection.cursor()\n",
    "                        query = f\"\"\"INSERT INTO \"HubSession\" (sequence,timestamp,source) VALUES (md5('%s'),current_timestamp,'{user}'); \"\"\" % (HubSessionDF['sequence'][i])\n",
    "                        cursor.execute(query)\n",
    "                        connection.commit() \n",
    "                        \n",
    "                SatSessionNameDF = input['SatSessionName']\n",
    "                for i in SatSessionNameDF.index:\n",
    "                        cursor = connection.cursor()\n",
    "                        query = f\"\"\"INSERT INTO \"SatSessionName\" (sequence,timestamp,source,name) VALUES (md5('%s'),current_timestamp,'{user}','%s'); \"\"\" % (SatSessionNameDF['sequence'][i],SatSessionNameDF['name'][i])\n",
    "                        cursor.execute(query)\n",
    "                        connection.commit() \n",
    "                        \n",
    "                SessionMetaDataDF = input['SessionMetaData']\n",
    "                for i in SessionMetaDataDF.index:\n",
    "                        cursor = connection.cursor()\n",
    "                        query = f\"\"\"INSERT INTO \"SessionMetaData\" (sequence,timestamp,source,session,metadata) VALUES (md5('%s'),current_timestamp,'{user}',md5('%s'),md5('%s')); \"\"\" % (SessionMetaDataDF['sequence'][i],SessionMetaDataDF['session'][i],SessionMetaDataDF['metadata'][i])\n",
    "                        cursor.execute(query)\n",
    "                        connection.commit() \n",
    "                \n",
    "                HubObservationDF = input['HubObservation']\n",
    "                for i in HubObservationDF.index:\n",
    "                        cursor = connection.cursor()\n",
    "                        query = f\"\"\"INSERT INTO \"HubObservation\" (sequence,timestamp,source,\"collectedAtSession\") VALUES (md5('%s'),current_timestamp,'{user}',md5('%s')); \"\"\" % (HubObservationDF['sequence'][i],HubObservationDF['collectedAtSession'][i])\n",
    "                        cursor.execute(query)\n",
    "                        connection.commit() \n",
    "                \n",
    "                ObservationMetaDataDF = input['ObservationMetaData']\n",
    "                for i in ObservationMetaDataDF.index:\n",
    "                        cursor = connection.cursor()\n",
    "                        query = f\"\"\"INSERT INTO \"ObservationMetaData\" (sequence,timestamp,source,observation,metadata) VALUES (md5('%s'),current_timestamp,'{user}',md5('%s'),md5('%s')); \"\"\" % (ObservationMetaDataDF['sequence'][i],ObservationMetaDataDF['observation'][i],ObservationMetaDataDF['metadata'][i])\n",
    "                        cursor.execute(query)\n",
    "                        connection.commit()                 \n",
    "                \n",
    "                AttendsSessionDF = input['AttendsSession']\n",
    "                for i in AttendsSessionDF.index:\n",
    "                        cursor = connection.cursor()\n",
    "                        query = f\"\"\"INSERT INTO \"AttendsSession\" (sequence,timestamp,source,\"experimentalUnit\",\"group\",\"session\") VALUES (md5('%s'),current_timestamp,'{user}',md5('%s'),md5('%s'),md5('%s')); \"\"\" % (AttendsSessionDF['sequence'][i],AttendsSessionDF['experimentalUnit'][i],AttendsSessionDF['group'][i],AttendsSessionDF['session'][i])\n",
    "                        cursor.execute(query)\n",
    "                        connection.commit() \n",
    "                \n",
    "                SatObservationNameDF = input['SatObservationName']\n",
    "                for i in SatObservationNameDF.index:\n",
    "                        cursor = connection.cursor()\n",
    "                        query = f\"\"\"INSERT INTO \"SatObservationName\" (sequence,timestamp,source,name) VALUES (md5('%s'),current_timestamp,'{user}','%s'); \"\"\" % (SatObservationNameDF['sequence'][i],SatObservationNameDF['name'][i])\n",
    "                        cursor.execute(query)\n",
    "                        connection.commit() \n",
    "                \n",
    "                SatObservationValueDF = input['SatObservationValue']\n",
    "                for i in SatObservationValueDF.index:\n",
    "                        cursor = connection.cursor()\n",
    "                        query = f\"\"\"INSERT INTO \"SatObservationValue\" (sequence,timestamp,source,value,timestamps) VALUES (md5('%s'),current_timestamp,'{user}',array%s,array%s::timestamp[]); \"\"\" % (SatObservationValueDF['sequence'][i],SatObservationValueDF['value'][i],SatObservationValueDF['timestamps'][i])\n",
    "                        cursor.execute(query)\n",
    "                        connection.commit() \n",
    "\n",
    "            print(\"Inserted data successfully in PostgreSQL \")\n",
    "\n",
    "        except (Exception, Error) as error:\n",
    "            print(\"Error while connecting to PostgreSQL\", error)\n",
    "        finally:\n",
    "            if connection:\n",
    "                cursor.close()\n",
    "                connection.close()\n",
    "                print(\"PostgreSQL connection is closed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractTransformLoadHelper:\n",
    "    def main():\n",
    "        r=FileReader()\n",
    "        t=FileTransformer()\n",
    "        l=FileLoader()\n",
    "        \n",
    "        \n",
    "        transformedData = []\n",
    "        \n",
    "        vmDeoxyMetaData=[]\n",
    "        vmDeoxyData=[]\n",
    "        vmDeoxyFileNames=[]\n",
    "        #Vm Data\n",
    "        for fileName in glob.glob('data/VMData_Blinded/*_HBA_Probe1_Deoxy.csv'):\n",
    "            with open(fileName, 'r', errors=\"ignore\") as file:\n",
    "                metaData, data = r.readVMFile(file)\n",
    "                vmDeoxyMetaData.append(metaData)\n",
    "                vmDeoxyData.append(data)\n",
    "                vmDeoxyFileNames.append(fileName)\n",
    "        \n",
    "        transformedVmDeoxyData = t.transformVMFile(vmDeoxyFileNames, vmDeoxyMetaData, vmDeoxyData)\n",
    "        \n",
    "        transformedData.append(transformedVmDeoxyData)\n",
    "\n",
    "        vmOxyMetaData=[]\n",
    "        vmOxyData=[]\n",
    "        vmOxyFileNames=[]\n",
    "        for fileName in glob.glob('data/VMData_Blinded/*_HBA_Probe1_Oxy.csv'):\n",
    "            with open(fileName, 'r', errors=\"ignore\") as file:\n",
    "                metaData, data = r.readVMFile(file)\n",
    "                vmOxyMetaData.append(metaData)\n",
    "                vmOxyData.append(data)\n",
    "                vmOxyFileNames.append(fileName)\n",
    "        \n",
    "        transformedVmOxyData = t.transformVMFile(vmOxyFileNames, vmOxyMetaData, vmOxyData)\n",
    "        \n",
    "        transformedData.append(transformedVmOxyData)\n",
    "        \n",
    "        vmMesMetaData=[]\n",
    "        vmMesData=[]\n",
    "        vmMesFileNames=[]\n",
    "        for fileName in glob.glob('data/VMData_Blinded/*_MES_Probe1.csv'):\n",
    "            with open(fileName, 'r', errors=\"ignore\") as file:\n",
    "                metaData, data = r.readVMFile(file)\n",
    "                vmMesMetaData.append(metaData)\n",
    "                vmMesData.append(data)\n",
    "                vmMesFileNames.append(fileName)\n",
    "        \n",
    "        transformedVmMesData = t.transformVMFile(vmMesFileNames, vmMesMetaData, vmMesData)\n",
    "        \n",
    "        transformedData.append(transformedVmMesData)\n",
    "              \n",
    "        \n",
    "        preAutismMetaData=[]\n",
    "        preAutismFileNames=[]\n",
    "        preAutismData=[]\n",
    "        preAutismWavelengthOneData=[]\n",
    "        preAutismWavelengthTwoData=[]\n",
    "        preAutismEventonsData=[]\n",
    "        \n",
    "        # Pre Autism Data\n",
    "        for fileName in glob.glob('data/PreAutismData_Blinded/*_NormalConversation/*.dat'):\n",
    "            data = pd.DataFrame(np.genfromtxt(fileName))\n",
    "            preAutismData.append(data)\n",
    "            \n",
    "        for fileName in glob.glob('data/PreAutismData_Blinded/*_NormalConversation/*.wl1'):\n",
    "            data = pd.DataFrame(np.genfromtxt(fileName))\n",
    "            preAutismWavelengthOneData.append(data)\n",
    "          \n",
    "        for fileName in glob.glob('data/PreAutismData_Blinded/*_NormalConversation/*.wl2'):\n",
    "            data = pd.DataFrame(np.genfromtxt(fileName))\n",
    "            preAutismWavelengthTwoData.append(data)\n",
    "                    \n",
    "        for fileName in glob.glob('data/PreAutismData_Blinded/*_NormalConversation/*.evt'):\n",
    "            data = pd.DataFrame(np.genfromtxt(fileName))\n",
    "            preAutismEventonsData.append(data)\n",
    "             \n",
    "        for fileName in glob.glob('data/PreAutismData_Blinded/*_NormalConversation/*.hdr'):\n",
    "            with open(fileName, 'r', errors=\"ignore\") as file:\n",
    "                metaData = r.readPreAutismMetaData(file)\n",
    "                preAutismMetaData.append(metaData)\n",
    "                preAutismFileNames.append(fileName)\n",
    "         \n",
    "        for fileName in glob.glob('data/PreAutismData_Blinded/*_StressedConversation/*.dat'):\n",
    "            data = pd.DataFrame(np.genfromtxt(fileName))\n",
    "            preAutismData.append(data)\n",
    "         \n",
    "        for fileName in glob.glob('data/PreAutismData_Blinded/*_StressedConversation/*.wl1'):\n",
    "            data = pd.DataFrame(np.genfromtxt(fileName))\n",
    "            preAutismWavelengthOneData.append(data)\n",
    "         \n",
    "        for fileName in glob.glob('data/PreAutismData_Blinded/*_StressedConversation/*.wl2'):\n",
    "            data = pd.DataFrame(np.genfromtxt(fileName))\n",
    "            preAutismWavelengthTwoData.append(data)\n",
    "                \n",
    "        for fileName in glob.glob('data/PreAutismData_Blinded/*_StressedConversation/*.evt'):\n",
    "            data = pd.DataFrame(np.genfromtxt(fileName))  \n",
    "            preAutismEventonsData.append(data)     \n",
    "        \n",
    "        for fileName in glob.glob('data/PreAutismData_Blinded/*_StressedConversation/*.hdr'):\n",
    "            with open(fileName, 'r', errors=\"ignore\") as file:\n",
    "                metaData = r.readPreAutismMetaData(file)\n",
    "                preAutismMetaData.append(metaData)\n",
    "                preAutismFileNames.append(fileName)\n",
    "        \n",
    "        transformedData.append(t.transformPreAutismFile(preAutismFileNames, preAutismMetaData, preAutismData, preAutismWavelengthOneData, preAutismWavelengthTwoData, preAutismEventonsData))\n",
    "        \n",
    "        l.loadDataToEnterpriseLayer(transformedData)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vijay\\AppData\\Local\\Temp\\ipykernel_32480\\991975479.py:75: UserWarning: genfromtxt: Empty input file: \"data/PreAutismData_Blinded\\Autism0002-1_NormalConversation\\NIRS-2019-10-16_005.evt\"\n",
      "  data = pd.DataFrame(np.genfromtxt(fileName))\n",
      "C:\\Users\\vijay\\AppData\\Local\\Temp\\ipykernel_32480\\991975479.py:97: UserWarning: genfromtxt: Empty input file: \"data/PreAutismData_Blinded\\Autism0002-2_StressedConversation\\NIRS-2019-10-16_004.evt\"\n",
      "  data = pd.DataFrame(np.genfromtxt(fileName))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted data successfully in PostgreSQL \n",
      "PostgreSQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ExtractTransformLoadHelper.main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('data_science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66aefaa15347530fd336e5ff4c6baa78eb01ac4bdb41d42482c99492d43d4e3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
